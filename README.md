# üï∑Ô∏èüìö Raspe um Di√°rio Oficial e contribua com o Querido Di√°rio

O [Querido Di√°rio](https://queridodiario.ok.org.br/) √© um projeto de c√≥digo aberto da [Open Knowledge Brasil](https://ok.org.br/) que utiliza Python e outras tecnologias para libertar informa√ß√µes do Di√°rio Oficial (DO) das administra√ß√µes p√∫blicas no Brasil. A iniciativa mapeia, baixa e converte todas as p√°ginas das publica√ß√µes para um formato mais acess√≠vel, a fim de facilitar a an√°lise de dados.

Neste tutorial, mostraremos algumas orienta√ß√µes gerais para construir um raspador e contribuir com o projeto Querido Di√°rio. 

## Colabore com o tutorial

Este √© reposit√≥rio ainda est√° em fase de elabora√ß√£o. Abaixo, est√£o algumas tarefas ainda pendentes. Voc√™ pode ajudar melhorando a documenta√ß√£o por meio de *pull requests* neste reposit√≥rio. Confira a lista de tarefas pendentes no final do documento.

Se voc√™ prefere uma apresenta√ß√£o sobre o projeto em v√≠deo, confira o workshop [Querido Di√°rio: hoje eu tornei um Di√°rio Oficial acess√≠vel](https://escoladedados.org/coda2020/workshop-querido-diario/) da Ana Paula Gomes no Coda.Br 2020.

## üîé Mapeando os Di√°rios Oficiais
Existem formas de colaborar com o Querido Di√°rio sem precisar programar. Voc√™ pode participar de nosso Censo, por exemplo, e ajudar a mapear os Di√°rios Oficiais de todos os munic√≠pios brasileiros.

Se voc√™ quiser botar a m√£o na massa e construir seu raspador, pode come√ßar ‚Äúadotando‚Äù uma cidade. Primeiro, encontre uma cidade que ainda n√£o esteja listado no [arquivo CITIES.md do reposit√≥rio](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md). 

O endere√ßo do reposit√≥rio do projeto √©: https://github.com/okfn-brasil/querido-diario/

Para acompanhar o tutorial e construir um raspador, √© necess√°rio algum conhecimento sobre:

- Uso do terminal
- Python e o pacote Scrapy
- Git e Github
- HTML,CSS, XPath

### Pareceu grego?

Se voc√™ n√£o se sente confort√°vel com estas tecnologias, sugerimos a leitura dos seguintes tutoriais primeiro.

- [Tutorial da documenta√ß√£o do Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)

- [Introdu√ß√£o a XPath para raspagem de dados](https://escoladedados.org/tutoriais/xpath-para-raspagem-de-dados-em-html/)


## üå± Configurando um ambiente de desenvolvimento
Fa√ßa um fork do reposit√≥rio oficial do Querido Di√°rio na sua conta no Github.

Em seguida, clone este novo reposit√≥rio para seu computador.

Se voc√™ usa Windows, baixe as [Ferramentas de Build do Visual Studio](https://visualstudio.microsoft.com/pt-br/downloads/#build-tools-for-visual-studio-2019) e execute o instalador. Durante a instala√ß√£o, selecione a op√ß√£o ‚ÄúDesenvolvimento para desktop com C++‚Äù e finalize o processo.

Se voc√™ usa Linux ou Mac Os, pode simplesmente executar os seguintes comandos. Eles tamb√©m est√£o descritos no README do projeto, na parte de configura√ß√£o de ambiente.

```
python3 -m venv .venv
source .venv/bin/activate
pip install -r data_collection/requirements.txt
pre-commit install
```

Usu√°rios de Windows devem executar os mesmo comandos, apenas trocando o segundo deles por:  `.venv\Scripts\activate.bat`


## üï∑ Conhecendo os raspadores

Todos os raspadores do projeto ficam na pasta [data_collection/gazette/spiders/](https://github.com/okfn-brasil/querido-diario/tree/main/data_collection/gazette/spiders). Navegue por diferentes arquivos e repare no que h√° de comum e diferente no c√≥digo de cada um.

Os nomes de todos os arquivos seguem o padr√£o: **uf_nomedacidade.py**. 

Ou seja, primeiro, temos a sigla da UF, seguido de underline e nome da cidade. Tudo em min√∫sculas, sem espa√ßos, acentos ou caracteres especiais.

Veja alguns exemplos paradigm√°ticos de Di√°rios Oficiais:

* **Pagina√ß√£o**: um bom exemplo de raspador de DO onde as publica√ß√µes est√£o separadas em v√°rias p√°ginas √© o [script da cidade de Manaus](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/am_manaus.py).

* **Busca de datas**: outra situa√ß√£o comum √© quando voc√™ precisa preencher um formul√°rio e fazer uma busca de datas para acessar as publica√ß√µes. √â caso por exemplo do script [ba_salvador.py](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_salvador.py), que raspa as informa√ß√µes da capital baiana.

* **Consulta via APIs**: pode ser tamb√©m que os dados sobre as publica√ß√µes estejam dispon√≠veis via API, j√° organizados em um arquivo JSON. √â o caso do raspador de [Natal](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/rn_natal.py).

Se voc√™ navegou pelos raspadores, talvez tenha reparado que alguns c√≥digos possuem apenas os metadados. Neste caso, tratam-se de munic√≠pios que compartilham o mesmo sistema de publica√ß√£o. Ent√£o, tratamos eles conjuntamente, como associa√ß√µes de munic√≠pios, ao inv√©s de repetir o mesmo raspador em cada arquivo.

Mas n√£o se preocupe com isso, por ora. Vamos voltar ao nosso exemplo e ver como construir um raspador completo individualmente.

## üß† Anatomia de um raspador

![Script b√°sico, com exemplo de Paul√≠nia](img/sp_paulinia.png)

<!-- Imagem gerada no site carbon.now.sh -->

Por padr√£o, todos os raspadores come√ßam importando alguns pacotes. Vejamos quais s√£o.

`import datetime`: pacote para lidar com datas.

`import scrapy`: quem faz quase toda m√°gica acontecer. √â o pacote utilizado para construir nossos raspadores.

`from gazette.spiders.base import BaseGazetteSpider`: √© o raspador (spider) base do projeto, que j√° traz v√°rias funcionalidades √∫teis.

### Par√¢metros inicias

Cada raspador traz uma classe em Python, que executa determinadas rotinas para cada URL de Di√°rios Oficiais. Todas as classes possuem pelo menos as informa√ß√µes b√°sicas abaixo.

Vejamos um exemplo a partir da cidade Paul√≠nia em S√£o Paulo.

`name` = Nome do raspador no mesmo padr√£o do nome do arquivo, sem a extens√£o. Exemplo: `sp_paulinia`.

`TERRITORY_ID` = c√≥digo da cidade no IBGE. Confira esta tabela da Wikipedia para descobrir o c√≥digo da sua cidade. Exemplo: `2905206`.

`allowed_domains` = Dom√≠nios nos quais o raspador ir√° atuar. Exemplo: `["www.paulinia.sp.gov.br/"]`

`start_urls` = URL de in√≠cio da navega√ß√£o do raspador. A resposta dessa requisi√ß√£o inicial √© encaminhada para a vari√°vel response, do m√©todo padr√£o do Scrapy chamado parse. Veremos mais sobre isso em breve. Exemplo:`["http://www.paulinia.sp.gov.br/semanarios/"]`

`start_date` = Representa√ß√£o de data no formato ano, m√™s e dia (YYYY, M, D), usando o pacote datetime. √â a data inicial da publica√ß√£o do Di√°rio Oficial no sistema quest√£o, ou seja, a data da primeira publica√ß√£o dispon√≠vel online. Encontre esta data pesquisando e inserindo essa data manualmente nesta vari√°vel. Exemplo: `datetime.date(2017, 4, 3)`.

### Par√¢metros de sa√≠da

Al√©m disso, cada raspador tamb√©m precisa retornar algumas informa√ß√µes por padr√£o. Isso acontece usando a fun√ß√£o `yield`.

`date` = A data da publica√ß√£o em quest√£o. Em nosso c√≥digo de exemplo, definimos este par√¢metro como o dia de hoje, apenas para ter uma vers√£o b√°sica operacional do c√≥digo. Por√©m, ao construir um raspador real, neste par√¢metro voc√™ dever√° indicar as datas corretas das publica√ß√µes.

`file_urls` = Retorna a URL das publica√ß√µes do DO. 

`power` = Aceita os par√¢metros `executive` ou `executive_legislative`. Aqui, definimos se o DO tem informa√ß√µes apenas do poder executivo ou tamb√©m do legislativo. Para definir isso, √© preciso olhar manualmente nas publica√ß√µes se h√° informa√ß√µes da C√¢mara Municipal agregadas no mesmo documento, por exemplo.

`is_extra_edition` = Sinalizamos aqui se √© uma edi√ß√£o extra do Di√°rio Oficial ou n√£o.

`edition_number` = N√∫mero da edi√ß√£o do DO em quest√£o.

Vejamos novamente nosso c√≥digo de exemplo.

# üëã Hello world: fa√ßa sua primeira requisi√ß√£o

O Scrapy come√ßa fazendo uma requisi√ß√£o para a URL definida no par√¢metro `start_urls`. A resposta dessa requisi√ß√£o vai para o m√©todo padr√£o `parse`, que ir√° armazenar a resposta na vari√°vel `response`.

A vari√°vel `response` tem v√°rios atributos, como o `text`, que traz o HTML da p√°gina em quest√£o como uma *string*.

Ent√£o, voc√™ pode uma forma de fazer um famoso "Hello, world!" no projeto Querido Di√°rio seria com um c√≥digo mais ou menos como este abaixo. Voc√™ encontra o c√≥digo visto acima no arquivo [sp_paulinia.py](sp_paulina.py), presente neste reposit√≥rio. Este c√≥digo n√£o baixa nenhum DO de fato, mas d√° as bases para voc√™ entender como os raspadores operam e por onde come√ßar a desenvolver o seu pr√≥prio.

Para testar um raspador e come√ßar a desenvolver o seu, siga as seguintes etapas:

1. Importe o arquivo para a pasta `data_collection/gazette/spiders/` no reposit√≥rio criado no seu computador.
2. Abra o terminal nesta pasta.
3. Ative o ambiente virtual, caso n√£o tenha feito antes. Rode `source .venv/bin/activate` ou o comando adequado na pasta onde o ambiente foi criado.
4. No terminal, rode o raspador com o comando `scrapy crawl nomedoraspador`. Ou seja, no exemplo rodamos: `scrapy crawl sp_paulinia`.

# üìÑ Dissecando o arquivo log

Se tudo deu certo, deve aparecer um arquivo de log enorme terminal. 

Ele come√ßa com **[scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: gazette)** e traz uma s√©rie de informa√ß√µes sobre o ambiente inicialmente. Mas a parte que mais nos interessa come√ßa apenas ap√≥s a linha **[scrapy.core.engine] INFO: Spider opened** e termina na linha **[scrapy.core.engine] INFO: Closing spider (finished)**. Vejamos abaixo.

![](img/output1.png)

A linha `DEBUG: Scraped from <200 http://www.paulinia.sp.gov.br/semanarios/>` nos indica conseguimos acessar o endere√ßo especificado (c√≥digo 200).

Ao desenvolvedor um raspador, busque principalmente por avisos de *WARNING* e *ERROR*. S√£o eles que trar√£o as informa√ß√µes mais importantes para voc√™ entender os problemas que ocorrem.

Depois de encerrado o raspador, temos a linha a se√ß√£o do *MONITORS*, que trar√° um relat√≥rio de execu√ß√£o. √â normal que apare√ßam erros, como este abaixo.

```
======================================================================
FAIL: Comparison Between Executions/Days without gazettes
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/home/abitporu/querido-diario/data_collection/gazette/monitors.py", line 66, in test_days_without_gazettes
    self.assertNotEqual(
**AssertionError: 0 == 0 : No gazettes scraped in the last 5 days.**

2021-08-19 18:44:04 [sp_paulinia] INFO: [Spidermon] 5 monitors in 0.022s
2021-08-19 18:44:04 [sp_paulinia] INFO: [Spidermon] FAILED (failures=1)
2021-08-19 18:44:04 [sp_paulinia] INFO: [Spidermon] -------------------------- FINISHED ACTIONS --------------------------
2021-08-19 18:44:04 [spidermon.contrib.actions.telegram] INFO: *sp_paulinia* finished
- Finish time: *2021-08-19 21:44:04.450166*
- Gazettes scraped: *1*
- üî• 1 failures üî•
```

Basicamente, estamos sendo avisados que nada foi raspado nos √∫ltimos dias. Tudo bem, este √© apenas um teste inicial para irmos nos familiarizando com o projeto.

# üõ†Ô∏è Construindo um raspador de verdade

Aqui, tudo vai depender da forma como cada site √© constru√≠do. Mas separamos algumas dicas gerais que podem te ajudar.

Primeiro, identifique um seletor que retorne todas as publica√ß√µes separadamente. Se as publica√ß√µes est√£o separadas em v√°rias abas ou v√°rias p√°ginas, primeiro certifique-se de que todas elas seguem o mesmo padr√£o. Sendo o caso, ent√£o, voc√™ pode come√ßar fazendo o raspador para a p√°gina mais recente e depois repetir as etapas para as demais, por meio de um loop, por exemplo.

Para testar os seletores e construir o raspador, voc√™ pode utilizar algumas destas alternativas:

* Inspetor Web: dispon√≠vel nos navegadores, permite a busca por seletores XPath.
  
* Scrapy shell: voc√™ tamb√©m pode testar seus seletores usando o Scrapy Shell. Experimente rodar por exemplo `scrapy shell "http://www.paulinia.sp.gov.br/semanarios"`. Neste terminal, voc√™ pode rodar c√≥digos como `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]")` e ver os resultados.

* Python debuger: insira a linha `import pdb; pdb.set_trace()` em meio a um loop para testar seu c√≥digo durante a execu√ß√£o.

# Enviando sua contribui√ß√£o

## Fazendo um commit

## Compartilhando um raspador parcialmente completo

## Compartilhando um raspador completo

# Tarefas pendentes

Se tiver d√∫vidas sobre algo, abra uma **issue** neste reposit√≥rio.

- [ ] Completar a lista de tutoriais introdut√≥rios com materiais relevantes
- [ ] Testar e reportar eventuais problemas com a configura√ß√£o de ambiente no Windows
- [ ] Testar e reportar eventuais problemas com a configura√ß√£o de ambiente no Linux
- [ ] Testar e reportar eventuais problemas com a configura√ß√£o de ambiente no Mac OS
- [ ] Documentar o processo de fazer um commit no reposit√≥rio e problemas comuns
- [ ] Fazer uma se√ß√£o mostrando como enviar o seu raspador depois de feito
- [ ] Melhorar a se√ß√£o "Construindo um raspador de verdade".
- [ ] Melhorar dicas para debugar o c√≥digo.
- [ ] Revisar e incorporar conte√∫dos faltantes (e ainda atuais) citados no artigo do [Vanz](http://jvanz.com/como-funciona-o-robozinho-do-serenata-que-baixa-os-diarios-oficiais.html).
- [ ] Revisar e incorporar conte√∫dos faltantes (e ainda atuais) citados no [post](https://www.anapaulagomes.me/pt-br/2020/10/).quero-tornar-di%C3%A1rios-oficiais-acess%C3%ADveis.-como-come%C3%A7ar/) e na apresenta√ß√£o no [Coda.Br 2020](https://escoladedados.org/coda2020/workshop-querido-diario/) feito por Ana Paula Gomes.
