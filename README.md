# üï∑Ô∏èüìö Raspe um Di√°rio Oficial e contribua com o Querido Di√°rio

O [Querido Di√°rio](https://queridodiario.ok.org.br/) √© um projeto de c√≥digo aberto da [Open Knowledge Brasil](https://ok.org.br/) que utiliza Python e outras tecnologias para libertar informa√ß√µes do Di√°rio Oficial (DO) das administra√ß√µes p√∫blicas no Brasil. A iniciativa mapeia, baixa e converte todas as p√°ginas das publica√ß√µes para um formato mais acess√≠vel, a fim de facilitar a an√°lise de dados.

Neste tutorial, mostraremos orienta√ß√µes gerais para construir um raspador e contribuir com o projeto Querido Di√°rio.

## üí™ Colabore com o tutorial

Este reposit√≥rio est√° em constru√ß√£o. No fim do documento, listamos algumas tarefas ainda pendentes. Voc√™ pode ajudar melhorando a documenta√ß√£o fazendo *pull requests* neste reposit√≥rio.

Se voc√™ prefere uma apresenta√ß√£o sobre o projeto em v√≠deo, confira o workshop [Querido Di√°rio: hoje eu tornei um Di√°rio Oficial acess√≠vel](https://escoladedados.org/coda2020/workshop-querido-diario/) da Ana Paula Gomes no [Coda.Br 2020](https://escoladedados.org/coda2020). Ainda que mudan√ßas recentes possam ter alterado detalhes apresentados na oficina, o v√≠deo √© uma √≥tima complementa√ß√£o a este tutorial. Voc√™ pode utilizar a *timestamp* na descri√ß√£o do v√≠deo para assistir apenas trechos de seu interesse.

## üîé Mapeando e escolhendo Di√°rios Oficiais

Existem formas de colaborar com o Querido Di√°rio sem precisar programar. Voc√™ pode participar de nosso [Censo](https://censo.ok.org.br/), por exemplo, e ajudar a mapear os Di√°rios Oficiais de todos os munic√≠pios brasileiros.

Se voc√™ quiser botar a m√£o na massa e construir seu raspador, pode come√ßar ‚Äúadotando‚Äù uma cidade. Primeiro, encontre uma cidade que ainda n√£o esteja listada no [arquivo CITIES.md do reposit√≥rio](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md).

O endere√ßo do reposit√≥rio do projeto √©: https://github.com/okfn-brasil/querido-diario/

Antes de come√ßar a trabalhar, vale tamb√©m dar uma olhada na se√ß√£o [Issues](https://github.com/okfn-brasil/querido-diario/issues) e [Pull Requests](https://github.com/okfn-brasil/querido-diario/pulls). Assim, voc√™ consegue checar se j√° existe um raspador para a cidade escolhida que ainda n√£o tenha sido incorporado ao projeto (*Pull Requests*) ou se h√° outras pessoas trabalhando no c√≥digo para o munic√≠pio (*Issues*).

Se o raspador da sua cidade n√£o consta como feito no [arquivo CITIES.md do reposit√≥rio](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md), n√£o est√° na se√ß√£o [Issues](https://github.com/okfn-brasil/querido-diario/issues), nem na aba de [Pull Requests](https://github.com/okfn-brasil/querido-diario/pulls), ent√£o, crie uma *Issue* nova para anunciar que voc√™ ir√° trabalhar no raspador da cidade escolhida.

## üíª Construindo o raspador

Para acompanhar o tutorial e construir um raspador, √© necess√°rio instalar e conhecer algo sobre os seguintes softwares:

- Uso do terminal
- [Python](https://www.python.org/) e o pacote Scrapy
- [Git](https://git-scm.com/) e Github
- HTML,CSS, XPath

Se voc√™ n√£o se sente confort√°vel com estas tecnologias, sugerimos a leitura dos seguintes tutoriais primeiro.

- [Tutorial da documenta√ß√£o do Scrapy](https://docs.scrapy.org/en/latest/intro/tutorial.html)

- [Introdu√ß√£o a XPath para raspagem de dados](https://escoladedados.org/tutoriais/xpath-para-raspagem-de-dados-em-html/)

- [Git Handbook](https://guides.github.com/introduction/git-handbook/)

## üå± Configurando um ambiente de desenvolvimento

[Fa√ßa um fork do reposit√≥rio](https://docs.github.com/pt/github/getting-started-with-github/quickstart/fork-a-repo) do Querido Di√°rio na sua conta no Github.

Em seguida, clone este novo reposit√≥rio para seu computador e crie uma nova branch para a cidade que ir√° trabalhar: `git checkout -b <sigladoestado>-<cidade>`

Vejamos um exemplo com a cidade Paul√≠nia em S√£o Paulo: `git checkout -b sp-paulinia`

Se voc√™ usa Windows, baixe as [Ferramentas de Build do Visual Studio](https://visualstudio.microsoft.com/pt-br/downloads/#build-tools-for-visual-studio-2019) e execute o instalador. Durante a instala√ß√£o, selecione a op√ß√£o ‚ÄúDesenvolvimento para desktop com C++‚Äù e finalize o processo.

Se voc√™ usa Linux ou Mac Os, pode simplesmente executar os seguintes comandos. Eles tamb√©m est√£o descritos no README do projeto, na parte de configura√ß√£o de ambiente.

```
python3 -m venv .venv
source .venv/bin/activate
pip install -r data_collection/requirements.txt
pre-commit install
```

Usu√°rios de Windows devem executar os mesmo comandos, apenas trocando o segundo deles por:  `.venv\Scripts\activate.bat`

## üï∑ Conhecendo os raspadores

Todos os raspadores do projeto ficam na pasta [data_collection/gazette/spiders/](https://github.com/okfn-brasil/querido-diario/tree/main/data_collection/gazette/spiders). Navegue por diferentes arquivos e repare no que h√° de comum e diferente no c√≥digo de cada um.

Os nomes de todos os arquivos seguem o padr√£o: **uf_nomedacidade.py**.

Ou seja, primeiro, temos a sigla da UF, seguido de underline e nome da cidade. Tudo em min√∫sculas, sem espa√ßos, acentos ou caracteres especiais.

Existem casos onde diversos munic√≠pios usam a mesma plataforma para publicar seus Di√°rios Oficiais. Nestas ocasi√µes, voc√™ ir√° encontrar um √∫nico raspador para todos eles. √â o caso, por exemplo, do arquivo [**ba_associacao_municipios.py**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_associacao_municipios.py).

Para se familiarizar, sugerimos que voc√™ navegue por alguns exemplos paradigm√°ticos de Di√°rios Oficiais:

* **Pagina√ß√£o**: um bom exemplo de raspador de DO onde as publica√ß√µes est√£o separadas em v√°rias p√°ginas √© o [script da cidade de Manaus](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/am_manaus.py).

* **Busca de datas**: outra situa√ß√£o comum √© quando voc√™ precisa preencher um formul√°rio e fazer uma busca de datas para acessar as publica√ß√µes. √â caso por exemplo do script [ba_salvador.py](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_salvador.py), que raspa as informa√ß√µes da capital baiana.

* **Consulta via APIs**: pode ser tamb√©m que ao analisar as requisi√ß√µes do site, voc√™ descubra uma API escondida, com dados dos documentos j√° organizadas em um arquivo JSON, por exemplo. √â o caso do raspador de [Natal](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/rn_natal.py).

Se voc√™ navegou pelos raspadores, talvez tenha reparado que alguns raspadores praticamente n√£o possuem c√≥digo e quase se repetem entre si. Neste caso, tratam-se de munic√≠pios que compartilham o mesmo sistema de publica√ß√£o. Ent√£o, tratamos eles conjuntamente, como associa√ß√µes de munic√≠pios, ao inv√©s de repetir o mesmo raspador em cada arquivo.

Mas n√£o se preocupe com isso, por ora. Vamos voltar ao nosso exemplo e ver como construir um raspador completo para apenas uma cidade.

## üß† Anatomia de um raspador

Por padr√£o, todos os raspadores come√ßam importando alguns pacotes. Vejamos quais s√£o.

`import datetime`: pacote para lidar com datas.

`from gazette.items import Gazette`: item que ser√° salvo com campos que devem/podem ser preenchidos com os metadados dos di√°rios.

`from gazette.spiders.base import BaseGazetteSpider`: √© o raspador (spider) base do projeto, que j√° traz v√°rias funcionalidades √∫teis.

### Par√¢metros inicias

Cada raspador traz uma classe em Python, que executa determinadas rotinas para cada URL de Di√°rios Oficiais. Todas as classes possuem pelo menos as informa√ß√µes b√°sicas abaixo.

`name` = Nome do raspador no mesmo padr√£o do nome do arquivo, sem a extens√£o. Exemplo: `sp_paulinia`.

`TERRITORY_ID` = c√≥digo da cidade no IBGE. Confira a o arquivo [`territories.csv`](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/resources/territories.csv) do projeto para descobrir o c√≥digo da sua cidade. Exemplo: `2905206`.

`allowed_domains` = Dom√≠nios nos quais o raspador ir√° atuar. Exemplo: `["paulinia.sp.gov.br"]`

`start_urls` = URL de in√≠cio da navega√ß√£o do raspador. A resposta dessa requisi√ß√£o inicial √© encaminhada para a vari√°vel response, do m√©todo padr√£o do Scrapy chamado `parse`. Veremos mais sobre isso em breve. Exemplo:`["http://www.paulinia.sp.gov.br/semanarios/"]`

`start_date` = Representa√ß√£o de data no formato ano, m√™s e dia (YYYY, M, D), usando o pacote `datetime`. √â a data inicial da publica√ß√£o do Di√°rio Oficial no sistema quest√£o, ou seja, a data da primeira publica√ß√£o dispon√≠vel online. Encontre esta data pesquisando e inserindo essa data manualmente nesta vari√°vel. Exemplo: `datetime.date(2017, 4, 3)`.

### Par√¢metros de sa√≠da

Al√©m disso, cada raspador tamb√©m precisa retornar algumas informa√ß√µes por padr√£o. Isso acontece usando a express√£o `yield`.

`date` = A data da publica√ß√£o em quest√£o. Em nosso c√≥digo de exemplo, definimos este par√¢metro como o dia de hoje, apenas para ter uma vers√£o b√°sica operacional do c√≥digo. Por√©m, ao construir um raspador real, neste par√¢metro voc√™ dever√° indicar as datas corretas das publica√ß√µes.

`file_urls` = Retorna a URL da publica√ß√£o do DO (um documento pode ter mais de uma URL, mas √© raro).

`power` = Aceita os par√¢metros `executive` ou `executive_legislative`. Aqui, definimos se o DO tem informa√ß√µes apenas do poder executivo ou tamb√©m do legislativo. Para definir isso, √© preciso olhar manualmente nas publica√ß√µes se h√° informa√ß√µes da C√¢mara Municipal agregadas no mesmo documento, por exemplo.

`is_extra_edition` = Sinalizamos aqui se √© uma edi√ß√£o extra do Di√°rio Oficial ou n√£o.

`edition_number` = N√∫mero da edi√ß√£o do DO em quest√£o.

Vejamos novamente nosso c√≥digo de exemplo.

# üëã Hello world: fa√ßa sua primeira requisi√ß√£o

O Scrapy come√ßa fazendo uma requisi√ß√£o para a URL definida no par√¢metro `start_urls`. A resposta dessa requisi√ß√£o vai para o m√©todo padr√£o `parse`, que ir√° armazenar a resposta na vari√°vel `response`.

Ent√£o, uma forma de fazer um "Hello, world!" no projeto Querido Di√°rio seria com um c√≥digo como este abaixo. 

```
import datetime
from gazette.items import Gazette
from gazette.spiders.base import BaseGazetteSpider

class SpPauliniaSpider(BaseGazetteSpider):
    name = "sp_paulinia"
    TERRITORY_ID = "2905206"
    start_date = datetime.date(2010, 1, 4)
    allowed_domains = ["paulinia.sp.gov.br"]
    start_urls = ["http://www.paulinia.sp.gov.br/semanarios/"]

    def parse(self, response):
        yield Gazette(
            date=datetime.date.today(),
            file_urls=[response.url],
            power="executive",
        )

```

Ele n√£o baixa nenhum DO de fato, mas d√° as bases para voc√™ entender como os raspadores operam e por onde come√ßar a desenvolver o seu pr√≥prio.

Para testar um raspador e come√ßar a desenvolver o seu, siga as seguintes etapas:

1. Importe o arquivo para a pasta `data_collection/gazette/spiders/` (ou crie um novo) no reposit√≥rio criado no seu computador a partir do seu fork do Querido Di√°rio.
2. Abra o terminal nesta pasta.
3. Ative o ambiente virtual, caso n√£o tenha feito antes. Rode `source .venv/bin/activate` ou o comando adequado na pasta onde o ambiente foi criado.
4. No terminal, rode o raspador com o comando `scrapy crawl nomedoraspador`. Ou seja, no exemplo rodamos: `scrapy crawl sp_paulinia`.

# üìÑ Dissecando o arquivo log

Se tudo deu certo, deve aparecer um arquivo de log enorme terminal.

Ele come√ßa com `[scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: gazette)` e traz uma s√©rie de informa√ß√µes sobre o ambiente inicialmente. Mas a parte que mais nos interessa come√ßa apenas ap√≥s a linha `[scrapy.core.engine] INFO: Spider opened` e termina na linha `[scrapy.core.engine] INFO: Closing spider (finished)`. Vejamos abaixo.

![](img/output1.png)

A linha `DEBUG: Scraped from <200 http://www.paulinia.sp.gov.br/semanarios/>` nos indica conseguimos acessar o endere√ßo especificado (c√≥digo 200).

Ao desenvolvedor um raspador, busque principalmente por avisos de *WARNING* e *ERROR*. S√£o eles que trar√£o as informa√ß√µes mais importantes para voc√™ entender os problemas que ocorrem.

Depois de encerrado o raspador, temos a linha a se√ß√£o do *MONITORS*, que trar√° um relat√≥rio de execu√ß√£o. √â normal que apare√ßam erros, como este abaixo.

![Exemplo de erro do Scrapy](img/scrapy_erro.png)
<!-- Imagem gerada no site carbon.now.sh -->

Basicamente, estamos sendo avisados que nada foi raspado nos √∫ltimos dias. Tudo bem, este √© apenas um teste inicial para irmos nos familiarizando com o projeto.

# üõ†Ô∏è Construindo um raspador de verdade

Aqui, tudo vai depender da forma como cada site √© constru√≠do. Mas separamos algumas dicas gerais que podem te ajudar.

Primeiro, identifique um seletor que retorne todas as publica√ß√µes separadamente. Se as publica√ß√µes est√£o separadas em v√°rias abas ou v√°rias p√°ginas, primeiro certifique-se de que todas elas seguem o mesmo padr√£o. Sendo o caso, ent√£o, voc√™ pode come√ßar fazendo o raspador para a p√°gina mais recente e depois repetir as etapas para as demais, por meio de um loop, por exemplo.

Como vimos, a vari√°vel `response` nos retorna todo conte√∫do da p√°gina inicial do nosso raspador. Ela tem v√°rios atributos, como o `text`, que traz todo HTML da p√°gina em quest√£o como uma *string*. Mas n√£o temos interesse em todo HTML da p√°gina, apenas em informa√ß√µes espec√≠ficas, ent√£o, todo trabalho da raspagem consiste justamenteem separar o joio do trigo, ou seja, utilizar seletores que v√£o nos permitir filtrar os dados de nosso interesse. Fazemos isso por meio de seletores CSS ou XPath.

## Identificando e testando os seletores

Uma forma f√°cil para testar os seletores do seu raspador √© usando o Scrapy Shell. Experimente rodar por exemplo o comando `scrapy shell "http://www.paulinia.sp.gov.br/semanarios"`. Agora, voc√™ pode interagir com a p√°gina por meio da linha de comando e deve ver os comandos que temos dispon√≠veis.

![Output do Scrapy Shell](img/scrapy_shell.png)
<!-- Imagem gerada no site carbon.now.sh -->

O elemento mais importante no nosso caso √© o `response`. Se o acesso ao site foi feito com √™xito, este comando dever√° retornar o c√≥digo 200.

J√° o comando `response.css("a")` nos retornaria informa√ß√µes sobre todos os links das p√°gina em quest√£o. Tamb√©m √© poss√≠vel usar o `response.xpath` para identificar os seletores.

O modo mais f√°cil para de fato identificar os tais seletores que iremos utilizar √© por meio do "Inspetor Web". Trata-se de uma fun√ß√£o dispon√≠vel em praticamente todos navegadores navegadores modernos. Basta clicar do lado direito na p√°gina e selecionar a op√ß√£o "Inspecionar". Assim, podemos visualizar o c√≥digo HTML, copiar e buscar por seletores XPath e CSS.

Experimente rodar o comando `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href")` e ver os resultados. Este seletor XPath busca primeiro por tags `div` em qualquer lugar da p√°gina, que tenha uma classe chamada `container body-content`. Dentro destas tags, buscamos ent√£o por outras `div` com a classe `row`. E, em qualquer lugar dentro destas √∫ltimas, por fim, buscamos por tags `a` (links) cujo atributo `href` contenha a palavra `AbreSemanario` e pedimos para retornar o valor apenas do atributo `href`. Existem v√°rias formas de escrever seletores para o mesmo objeto. Voc√™ pode ter uma ideia de como montar o seletor inspecionando a p√°gina que disponibiliza os DOs.

Se voc√™ rodar o comando acima, ir√° ver uma lista de objetos como este: `<Selector xpath="//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href" data='AbreSemanario.aspx?id=1064'>`.

O que realmente nos interessa √© aquilo que est√° dentro do par√¢metro `data`, ou seja, o trecho da URL que nos permite acesso a cada publica√ß√£o. Ent√£o, adicione o `extract()` ao fim do comando anterior: `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href").extract()`.

Se o objetivo fosse selecionar apenas o primeiro item da lista, poder√≠amos usar o `.extract_first()`.

## Construindo o c√≥digo do raspador

Ap√≥s identificar os seletores, √© hora de construir seu raspador no arquivo `.py` da pasta `spiders`.

Para ajudar a debugar eventuais problemas na constru√ß√£o do c√≥digo, voc√™ pode inserir a linha `import pdb; pdb.set_trace()` em qualquer trecho do raspador para inspecionar seu c√≥digo (contexto, vari√°veis, etc.) durante a execu√ß√£o.

No final do processo, teste seu raspador usando o `scrapy crawl nomedoraspador`.

# Enviando sua contribui√ß√£o

Ao fazer o commit do c√≥digo, mencione a issue do raspador da sua cidade. Voc√™ pode incluir uma mensagem como `Close #20`, por exemplo. Onde #20 √© o n√∫mero identificador da issue criada. Tamb√©m adicione uma descri√ß√£o comentando suas op√ß√µes na hora de desenvolver o raspador ou eventuais incertezas.

# Tarefas pendentes

- [ ] Melhorar as se√ß√µes 'Enviando sua contribui√ß√£o' e 'Construindo o c√≥digo do raspador'
- [ ] Revisar e incorporar conte√∫dos faltantes (e atuais) citados no artigo do [Vanz](http://jvanz.com/como-funciona-o-robozinho-do-serenata-que-baixa-os-diarios-oficiais.html).
- [ ] Revisar e incorporar conte√∫dos faltantes (e atuais) citados no [post](https://www.anapaulagomes.me/pt-br/2020/10/quero-tornar-di%C3%A1rios-oficiais-acess%C3%ADveis.-como-come%C3%A7ar/) feito pela Ana Paula Gomes.
