[Portugu√™s (BR)](README.md)

# Raspe um Di√°rio Oficial e contribua com o Querido Di√°rio üï∑Ô∏èüìö

[Querido Di√°rio](https://queridodiario.ok.org.br/) is an open-source project by ‚ÄúOpen Knowledge Brasil‚Äù [Open Knowledge Brasil](https://ok.org.br/) that uses Python and other technologies to liberate information from the Official Gazette (DO) of public administrations in Brazil.  The initiative maps, downloads, and converts all pages of the publications into a more accessible format to facilitate data analysis.

In this tutorial, we will provide general guidelines for building a scraper and contributing to ‚ÄúQuerido Di√°rio‚Äù project.

If you prefer a video presentation about the project, feel free to check out the workshop [Querido Di√°rio: hoje eu tornei um Di√°rio Oficial acess√≠vel](https://escoladedados.org/coda2020/workshop-querido-diario/) taught by Ana Paula Gomes at [Coda.Br 2020](https://escoladedados.org/coda2020). Even though recent changes may have altered details presented in the workshop, the video is an excellent complement to this tutorial. You can use the *timestamp* in the video description to watch only the parts that interest you.

## Sum√°rio üìë
  1. [Collaborate in this tutorial](#collaborate-in-this-tutorial-)
  2. [Mapping and Choosing Official Gazettes](#mapping-and-choosing-official-gazettes-)
  3. [Building a scraper](#building-a-scraper-)
  4. [Setting up a development environment](#setting-up-a-development-environment-)
  5. [Getting to know the scrapers](#getting-to-know-the-scrapers-)
      1. [Specific cases](#casos-particulares)
  6. [Anatomy of a scraper](#anatomia-de-um-raspador-)
      1. [Initial parameters](#parametros-iniciais)
      2. [Output parameters](#parametros-de-saida)
  7. [Hello world: make your first request](#hello-world-faca-sua-primeira-requisicao-)
  8. [Dissecting the log](#dissecando-o-log-)
  9. [Building a real scraper](#construindo-um-raspador-de-verdade-)
      1. [Identifying and testing selectors](#identificando-e-testando-os-seletores)
      2. [Writing the scraper code](#construindo-o-codigo-do-raspador)
      3. [Hints to test a scraper](#dcas-para-testar-o-raspador)
  10. [Sending your contribution](#enviando-sua-contribuicao-)

## Collaborate in this tutorial üí™

This document is constantly under construction. You can help improve this documentation by making *pull requests* to this repository.

## Mapping and Choosing Official Gazettess üîé

There are ways to collaborate with Querido Di√°rio without having to program. You can participate in the [Census](https://censo.ok.org.br/), for example, and help map the Official Gazettes of all Brazilian municipalities.

If you want to get your hands dirty and build your scraper, you can start by ‚Äúadopting‚Äù a city. First, find a city that is not already listed in the [repository CITIES.md file](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md).

The project repository address is: https://github.com/okfn-brasil/querido-diario/

Before starting work, it's also worth taking a look at the [Issues](https://github.com/okfn-brasil/querido-diario/issues) and [Pull Requests](https://github.com/okfn/querido-diario/pulls). This way, you can check if there is already a scraper for the chosen city that has not yet been incorporated into the project (*Pull Requests*) or if there are other people working on the code for the municipality (*Issues*).

If your city's scraper does not appear as done in the [CITIES.md file in the repository](https://github.com/okfn-brasil/querido-diario/blob/main/CITIES.md), it is not in the section [ Issues](https://github.com/okfn-brasil/querido-diario/issues), nor in the [Pull requests](https://github.com/okfn-brasil/querido-diario/pulls) tab, So, create a new *Issue* to announce that you will work at the scraper in the chosen city.

## Building a scraper üíª

To follow this tutorial and build a scraper, you need to install and know something about the following softwares:

- Use of the terminal
- [Python](https://www.python.org/) and the Scrapy package
- [Git](https://git-scm.com/) and Github
- HTML, CSS, XPath

If you are not comfortable with these technologies, we suggest the following materials:

- [Python for zombies](https://www.youtube.com/watch?v=YO58tXerKDc&list=PLUukMN0DTKCtbzhbYe2jdF4cr8MOWClXc)

- [Scrapy Documentation Tutorial](https://docs.scrapy.org/en/latest/intro/tutorial.html)

- [Introduction to XPath for data scraping](https://escoladedados.org/tutoriais/xpath-para-raspagem-de-dados-em-html/)

- [Git Handbook](https://guides.github.com/introduction/git-handbook/)

## Configurando um ambiente de desenvolvimento üå±

[Fork the Querido Diario repository](https://docs.github.com/pt/github/getting-started-with-github/quickstart/fork-a-repo) in your Github account.

Then, clone this new repository to your computer and create a new branch for the city you will work on:

```sh
git checkout -b <state acronym>-<city>
```

Let's look at an example with the city Paul√≠nia in S√£o Paulo:

```sh
git checkout -b sp-paulinia
```

If you use Windows, download [Visual Studio Build Tools](https://visualstudio.microsoft.com/pt-br/downloads/#build-tools-for-visual-studio-2019) and run the installer. During installation, select the ‚ÄúDesktop development with C++‚Äù option and complete the process.

If you use Linux or Mac Os, you can simply run the following commands. They are also described in the project's README, in the environment configuration part.

```py
python3 -m venv .venv
source .venv/bin/activate
pip install -r data_collection/requirements.txt
pre-commit install
```

Windows users must execute the same commands, just replacing the `source .venv/bin/activate` with `.venv\Scripts\activate.bat`.

## Getting to know the scrapers üï∑

All project scrapers are located in the folder [data_collection/gazette/spiders/](https://github.com/okfn-brasil/querido-diario/tree/main/data_collection/gazette/spiders). Browse different files and notice what's common and different in each one's code.

The names of all files follow the pattern: **uf_nome_da_cidade.py**.

In other words, first, we have the UF acronym (federative unit, Unidade Federativa in portuguese), followed by _underscore_ and the name of the city. All in lowercase, without spaces, accents or special characters and separating words with _underscore_.

To familiarize yourself, we suggest you browse some paradigmatic examples of Official Gazettes:

* **Pagination**: a good example of a scraper where publications are separated into several pages is [from the city of Manaus](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection /gazette/spiders/am_manaus.py).

* **Data search**: another common situation is when you need to fill out a form and search for data to access publications. This is the case, for example, of the script [ba_salvador.py](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_salvador.py), which scrapes information from Bahia's capital.

* **Query via APIs**: also when analyzing the website's requests, you discover a hidden API, with document data already organized in a JSON file, for example. This is the case with the [Natal] scraper(https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/rn_natal.py). At Escola de Dados, you can find a webinar about [data scraping through "hidden APIs"](https://escoladedados.org/2021/05/como-descobrir-apis-escondidas-para-facilitar-a- data-scraping/), which can be useful for those just starting out.

### Specific cases

You may have noticed that some scrapers have practically no code and almost repeat themselves. In this case, these are municipalities that share the same publication system. So, we treat them together, modifying only what is necessary from scraper to scraper, instead of repeating the same code in each file. This is the case, for example, of cities in Santa Catarina such as [**Abdon Batista**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/sc_abdon_batista. py) and [**Agrol√¢ndia**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/sc_agrolandia.py).

There are scrapers that do not have a city name because several municipalities use the same platform to publish their Official Gazettes. They are usually municipal association websites. This is the case of [**ba_associacao_municipios.py**](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/spiders/ba_associacao_municipios.py).

But for a first contribution, don't worry about these particular cases. Let's go back to our example and see how to build a complete scraper for just one city.

## Anatomy of a scraper üß†

Por padr√£o, todos os raspadores come√ßam importando alguns pacotes. Vejamos quais s√£o:

* `import datetime`: pacote para lidar com datas.
* `from gazette.items import Gazette`: Chamamos de `Gazette` os arquivo de DOs encontrados pelos raspadores, ele ir√° armazenar tamb√©m campos de metadados para cada publica√ß√£o. 
* `from gazette.spiders.base import BaseGazetteSpider`: √© o raspador (spider) base do projeto, que j√° traz algumas funcionalidades √∫teis.

### Par√¢metros iniciais

Cada raspador traz uma [classe em Python](https://www.youtube.com/watch?v=52ns4X7Ny6k&list=PLUukMN0DTKCtbzhbYe2jdF4cr8MOWClXc&index=41), que executa determinadas rotinas para cada p√°gina dos sites que publicam Di√°rios Oficiais. Todas as classes possuem pelo menos as informa√ß√µes b√°sicas abaixo:

* `name`: Nome do raspador no mesmo padr√£o do nome do arquivo, sem a extens√£o. Exemplo: `sp_paulinia`.
* `TERRITORY_ID`: c√≥digo da cidade no IBGE. Confira a o arquivo [`territories.csv`](https://github.com/okfn-brasil/querido-diario/blob/main/data_collection/gazette/resources/territories.csv) do projeto para descobrir o c√≥digo da sua cidade. Exemplo: `2905206`.
* `allowed_domains`: Dom√≠nios nos quais o raspador ir√° atuar. Tenha aten√ß√£o aos colchetes. Eles indicam que se trata de uma lista, ainda que tenha apenas um elemento. Exemplo: `["paulinia.sp.gov.br"]`
* `start_urls`: lista com URLs de in√≠cio da navega√ß√£o do raspador (normalmente apenas uma). A resposta dessa requisi√ß√£o inicial √© encaminhada para a vari√°vel `response`, do m√©todo padr√£o do Scrapy chamado `parse`. Veremos mais sobre isso em breve. Novamente, aten√ß√£o aos colchetes. Exemplo:`["http://www.paulinia.sp.gov.br/semanarios/"]`
* `start_date`: Representa√ß√£o de data no formato ano, m√™s e dia (YYYY, M, D) com `datetime.date`. √â a data inicial da publica√ß√£o do Di√°rio Oficial no sistema quest√£o, ou seja, a data da primeira publica√ß√£o dispon√≠vel online. Encontre esta data pesquisando e a insira manualmente nesta vari√°vel. Exemplo: `datetime.date(2017, 4, 3)`.

### Par√¢metros de sa√≠da

Al√©m disso, cada raspador tamb√©m precisa retornar algumas informa√ß√µes por padr√£o. Isso acontece usando a express√£o `yield` nos itens criados do tipo `Gazette`.

* `date`: A data da publica√ß√£o do di√°rio. 
* `file_urls`: Retorna as URLs da publica√ß√£o do DO como uma lista. Um documento pode ter mais de uma URL, mas n√£o √© algo comum.
* `power`: Aceita os par√¢metros `executive` ou `executive_legislative`. Aqui, definimos se o DO tem informa√ß√µes apenas do poder executivo ou tamb√©m do legislativo. Para definir isso, √© preciso olhar manualmente nas publica√ß√µes se h√° informa√ß√µes da C√¢mara Municipal agregadas no mesmo documento, por exemplo.
* `is_extra_edition`: Sinalizamos aqui se √© uma edi√ß√£o extra do Di√°rio Oficial ou n√£o. Edi√ß√µes extras s√£o edi√ß√µes completas do di√°rio que s√£o publicadas fora do calend√°rio previsto.
* `edition_number`: N√∫mero da edi√ß√£o do DO em quest√£o.

Vejamos agora nosso c√≥digo de exemplo.

## Hello world: fa√ßa sua primeira requisi√ß√£o üëã

O Scrapy come√ßa fazendo uma requisi√ß√£o para a URL definida no par√¢metro `start_urls`. A resposta dessa requisi√ß√£o vai para o m√©todo padr√£o `parse`, que ir√° armazenar a resposta na vari√°vel `response`.

Ent√£o, uma forma de fazer um "Hello, world!" no projeto Querido Di√°rio seria com um c√≥digo como este abaixo.

```python
import datetime
from gazette.items import Gazette
from gazette.spiders.base import BaseGazetteSpider

class SpPauliniaSpider(BaseGazetteSpider):
    name = "sp_paulinia"
    TERRITORY_ID = "2905206"
    start_date = datetime.date(2010, 1, 4)
    allowed_domains = ["paulinia.sp.gov.br"]
    start_urls = ["http://www.paulinia.sp.gov.br/semanarios/"]

    def parse(self, response):
        yield Gazette(
            date=datetime.date.today(),
            file_urls=[response.url],
            power="executive",
        )
```

O c√≥digo baixa o HTML da URL inicial, mas n√£o descarrega nenhum DO de fato. Definimos este par√¢metro como o dia de hoje, apenas para ter uma vers√£o b√°sica operacional do c√≥digo. Por√©m, ao construir um raspador real, neste par√¢metro voc√™ dever√° indicar as datas corretas das publica√ß√µes.

De todo modo, isso d√° as bases para voc√™ entender como os raspadores operam e por onde come√ßar a desenvolver o seu pr√≥prio.

Para rodar o c√≥digo, voc√™ pode seguir as seguintes etapas:

1. Crie um arquivo na pasta `data_collection/gazette/spiders/` do reposit√≥rio criado no seu computador a partir do seu fork do Querido Di√°rio;
2. Abra o terminal na ra√≠z do projeto;
3. Ative o ambiente virtual, caso n√£o tenha feito antes, como indicado na se√ß√£o _"[Configurando um ambiente de desenvolvimento](#configurando-um-ambiente-de-desenvolvimento)"_ (`source .venv/bin/activate`, por exemplo);
4. No terminal, v√° para a pasta `data_collection`;
5. No terminal, rode o raspador com o comando `scrapy crawl nome_do_raspador` (nome que est√° no atributo `name` da classe do raspador). Ou seja, no exemplo rodamos: `scrapy crawl sp_paulinia`.

## Dissecando o log üìÑ

Se tudo deu certo, deve aparecer um log enorme terminal.

Ele come√ßa com algo como `[scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: gazette)` e traz uma s√©rie de informa√ß√µes sobre o ambiente inicialmente. Mas a parte que mais nos interessa come√ßa apenas ap√≥s a linha `[scrapy.core.engine] INFO: Spider opened` e termina na linha `[scrapy.core.engine] INFO: Closing spider (finished)`. Vejamos abaixo.

![](img/output1.png)

A linha `DEBUG: Scraped from <200 http://www.paulinia.sp.gov.br/semanarios/>` nos indica conseguimos acessar o endere√ßo especificado (c√≥digo 200).

Ao desenvolver um raspador, busque principalmente por avisos de *WARNING* e *ERROR*. S√£o eles que trar√£o as informa√ß√µes mais importantes para voc√™ entender os problemas que ocorrem.

Depois de encerrado o raspador, temos as linhas da se√ß√£o dos *monitors*, que trar√° um relat√≥rio de execu√ß√£o. √â normal que apare√ßam erros, como este abaixo.

![Exemplo de erro do Scrapy](img/scrapy_erro.png)
<!-- Imagem gerada no site carbon.now.sh -->

√â um aviso que nada foi raspado nos √∫ltimos dias. Tudo bem, este √© apenas um teste inicial para irmos nos familiarizando com o projeto.

## Construindo um raspador de verdade üõ†Ô∏è

Aqui, tudo vai depender da forma como cada site √© constru√≠do. Mas separamos algumas dicas gerais que podem te ajudar.

Primeiro, navegue pelo site para entender a forma como os DOs est√£o disponibilizados. Busque encontrar um padr√£o consistente e pouco sucet√≠vel a mudan√ßas ocasionais para o rob√¥ extrair as informa√ß√µes necess√°rias. Por exemplo, se as publica√ß√µes est√£o separadas em v√°rias abas ou v√°rias p√°ginas, primeiro certifique-se de que todas elas seguem o mesmo padr√£o. Sendo o caso, ent√£o, voc√™ pode come√ßar fazendo o raspador para a p√°gina mais recente e depois repetir as etapas para as demais, por meio de um loop, por exemplo.

Como vimos, a vari√°vel `response` nos retorna todo conte√∫do da p√°gina inicial do nosso raspador. Ela tem v√°rios atributos, como o `text`, que traz todo HTML da p√°gina em quest√£o como uma *string*. Mas n√£o temos interesse em todo HTML da p√°gina, apenas em informa√ß√µes espec√≠ficas, ent√£o, todo trabalho da raspagem consiste justamente em separar o joio do trigo para filtrar os dados de nosso interesse. Fazemos isso por meio de seletores CSS, XPath ou express√µes regulares.

### Identificando e testando os seletores

Uma forma f√°cil para testar os seletores do seu raspador √© usando o Scrapy Shell. Experimente rodar por exemplo o comando `scrapy shell "http://www.paulinia.sp.gov.br/semanarios"`. Agora, voc√™ pode interagir com a p√°gina por meio da linha de comando e deve ver os comandos que temos dispon√≠veis.

![Output do Scrapy Shell](img/scrapy_shell.png)
<!-- Imagem gerada no site carbon.now.sh -->

O elemento mais importante no nosso caso √© o `response`. Se o acesso ao site foi feito com √™xito, este comando dever√° retornar o c√≥digo 200.

J√° o comando `response.css("a")` nos retornaria informa√ß√µes sobre todos os links das p√°gina em quest√£o. Tamb√©m √© poss√≠vel usar o `response.xpath` para identificar os seletores.

O modo mais f√°cil para de fato identificar os tais seletores que iremos utilizar √© por meio do "Inspetor Web". Trata-se de uma fun√ß√£o dispon√≠vel em praticamente todos navegadores navegadores modernos. Basta clicar do lado direito na p√°gina e selecionar a op√ß√£o "Inspecionar". Assim, podemos visualizar o c√≥digo HTML, copiar e buscar por seletores XPath e CSS.

Experimente rodar o comando `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href")` e ver os resultados. Este seletor XPath busca primeiro por tags `div` em qualquer lugar da p√°gina, que tenha como classe `container body-content`. Dentro destas tags, buscamos ent√£o por outras `div` com a classe `row`. E, em qualquer lugar dentro destas √∫ltimas, por fim, buscamos por tags `a` (links) cujo atributo `href` contenha a palavra `AbreSemanario` e pedimos para retornar o valor apenas do atributo `href`. 

Existem v√°rias formas de escrever seletores para o mesmo objeto. Voc√™ pode ter uma ideia de como montar o seletor inspecionando a p√°gina que disponibiliza os DOs.

Se voc√™ rodar o comando acima, ir√° ver uma lista de objetos como este: `<Selector xpath="//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href" data='AbreSemanario.aspx?id=1064'>`.

O que realmente nos interessa √© aquilo que est√° dentro do par√¢metro `data`, ou seja, o trecho da URL que nos permite acesso a cada publica√ß√£o. Ent√£o, adicione o `getall()` ao fim do comando anterior: `response.xpath("//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]/@href").getall()`.

Se o objetivo fosse selecionar apenas o primeiro item da lista, poder√≠amos usar o `.get()`.

Por vezes, pode ser necess√°rio utilizar express√µes regulares (regex) para "limpar" os seletores. A [Escola de Dados tem um tutorial sobre o assunto](https://escoladedados.org/tutoriais/expressao-regular-pode-melhorar-sua-vida/) e voc√™ vai encontrar diversos outros materiais na internet com exemplos de regex comuns, como este que aborda [express√µes para identificar datas](https://www.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/ch04s04.html) - algo que pode ser muito √∫til na hora de trabalhar com DOs.

Ap√≥s identificar os seletores, √© hora de construir seu raspador no arquivo `.py` da pasta `spiders`.

### Construindo o c√≥digo do raspador

Normalmente, para completar o seu raspador voc√™ precisar√° fazer algumas requisi√ß√µes extras. √â poss√≠vel identificar quais requisi√ß√µes s√£o necess√°rias fazer atrav√©s do "Analizador de Rede" em navegadores. A [palestra do Giulio Carvalho na Python Brasil 2020](https://youtu.be/nhEPZ3r5zGY) mostra como pode ser feita essa an√°lise de requisi√ß√µes de um site para depois converter em um raspador para o Querido Di√°rio.

Se voc√™ precisar fazer alguma requisi√ß√£o `GET`, o objeto de requisi√ß√£o `scrapy.Request` deve ser o suficiente. O objeto `scrapy.FormRequest` normalmente √© usado para requisi√ß√µes `POST`, que enviam algum dado no `formdata`.

Sempre que uma requisi√ß√£o for feita a partir de uma p√°gina, ela √© feita utilizando a express√£o `yield` e sua resposta ser√° enviada para algum m√©todo da classe do raspador. Ou seja, al√©m de um item (Gazette), como j√° vimos, o `yield` pode retornar uma requisi√ß√£o para outra p√°gina. As requisi√ß√µes t√™m alguns par√¢metros essenciais (outros par√¢metros podem ser vistos na documenta√ß√£o do Scrapy):

* `url`: A URL da p√°gina que ser√° acessada;
* `callback`: O m√©todo da classe do raspador para o qual a resposta ser√° enviada (por padr√£o, o m√©todo `parse` √© utilizado);
* `formdata` (em `FormRequest`): Um dicion√°rio contendo campos e seus valores que ser√£o enviados.

No exemplo completo para a cidade de Paul√≠nia (SP), na p√°gina inicial temos links para todos os anos onde h√° di√°rios dispon√≠veis e em cada ano todos os di√°rios s√£o listados na mesma p√°gina. Ent√£o, uma requisi√ß√£o √© feita para acessar a p√°gina de cada ano (a p√°gina inicial j√° √© o ano atual) usando `scrapy.FormRequest` (nesse caso, um m√©todo `.from_response` que j√° aproveita muitas coisas da `response` atual, inclusive a pr√≥pria URL). A resposta dessa requisi√ß√£o deve ir para o m√©todo `parse_year` que ir√° extrair todos os metadados poss√≠veis de ser encontrados na p√°gina. Com isso, a extra√ß√£o dos di√°rios de Paul√≠nia est√° completa üòÑ.

Veja como fica o raspador no exemplo a seguir (com coment√°rios para explicar algumas partes do c√≥digo):

```python
# Importa√ß√£o dos pacotes necess√°rios
import datetime
import scrapy
from gazette.items import Gazette
from gazette.spiders.base import BaseGazetteSpider

# Defini√ß√£o da classe do raspador
class SpPauliniaSpider(BaseGazetteSpider):

    # Par√¢metros iniciais
    name = "sp_paulinia"
    TERRITORY_ID = "2905206"
    start_date = datetime.date(2010, 1, 4)
    allowed_domains = ["www.paulinia.sp.gov.br"]
    start_urls = ["http://www.paulinia.sp.gov.br/semanarios"]

    # O parse abaixo ir√° partir da start_url acima
    def parse(self, response):
        # Nosso seletor cria uma lista com os c√≥digo HTML onde os anos est√£o localizados
        years = response.css("div.col-md-1")
        # E fazer um loop para extrair de fato o ano
        for year in years:
            # Para cada item da lista (year) vamos pegar (get) um seletor XPath.
            # Tamb√©m dizemos que queremos o resultado como um n√∫mero inteiro (int)
            year_to_scrape = int(year.xpath("./a/font/text()").get())

            # Para n√£o fazer requisi√ß√µes desnecess√°rias, se o ano j√° for o da p√°gina
            # inicial (p√°gina inicial √© o ano atual) ou ent√£o for anterior ao ano da
            # data inicial da busca, n√£o iremos fazer a requisi√ß√£o
            if (
                year_to_scrape < self.start_date.year
                or year_to_scrape == datetime.date.today().year
            ):
                continue

            # Com Scrapy √© poss√≠vel utilizar regex direto no elemento com os m√©todos
            # `.re` e `.re_first` (na maioria das vezes √© suficiente e n√£o precisamos
            # usar m√©todos da biblioteca `re`)
            event_target = year.xpath("./a/@href").re_first(r"(ctl00.*?)',")

            # O m√©todo `.from_response` nesse caso √© bem √∫til pois pega v√°rios
            # elementos do tipo <input> que j√° est√£o dentro do elemento <form>
            # localizado na p√°gina e preenche eles automaticamente no formdata, assim
            # √© poss√≠vel economizar muitas linhas de c√≥digo
            yield scrapy.FormRequest.from_response(
                response,
                formdata={"__EVENTTARGET": event_target},
                callback=self.parse_year,
            )

        # O `yield from` permite fazermos `yield` em cada resultado do m√©todo gerador
        # `self.parse_year`, assim, aqui estamos dando `yield` em todos os itens
        # `Gazette` raspados da p√°gina inicial
        yield from self.parse_year(response)

    def parse_year(self, response):
        editions = response.xpath(
            "//div[@class='container body-content']//div[@class='row']//a[contains(@href, 'AbreSemanario')]"
        )

        for edition in editions:
            document_href = edition.xpath("./@href").get()

            title = edition.xpath("./text()")

            gazette_date = datetime.datetime.strptime(
                title.re_first(r"\d{2}/\d{2}/\d{4}"), "%d/%m/%Y"
            ).date()
            edition_number = title.re_first(r"- (\d+) -")
            is_extra_edition = "extra" in title.get().lower()

            # Esse site "esconde" o link direto do PDF por tr√°s de uma s√©rie de
            # redirecionamentos, por√©m, como nas configura√ß√µes do projeto √© permitido
            # que arquivos baixados sofram redirecionamento, √© poss√≠vel colocar o link
            # "falso" j√° no item `Gazette` e o projeto vai conseguir baixar o documento
            yield Gazette(
                date=gazette_date,
                edition_number=edition_number,
                file_urls=[response.urljoin(document_href)],
                is_extra_edition=is_extra_edition,
                power="executive",
            )
```
Para ajudar a debugar eventuais problemas na constru√ß√£o do c√≥digo, voc√™ pode inserir a linha `import pdb; pdb.set_trace()` em qualquer trecho do raspador para inspecionar seu c√≥digo (contexto, vari√°veis, etc.) durante a execu√ß√£o.


### Rodando o raspador
Para rodar o raspador, execute o seguinte comando no terminal:

```
scrapy crawl nome_do_raspador
```

No caso acima, seria:

```
scrapy crawl sp_paulinia
```
O comando acima ir√° baixar os arquivos dos Di√°rios Oficiais ir√° a pasta `data`. Durante o processo de desenvolvimento, muitas vezes √© √∫til usar tamb√©m os seguintes par√¢metros adicionais na hora de rodar o raspador:

- `-s FILES_STORE=""`: Testar o raspador sem baixar nenhum arquivo adicionando. Isso √© √∫til para testar r√°pido se todas as requisi√ß√µes est√£o funcionando.

- `-o output.csv`: Adiciona os itens extra√≠dos para um arquivo CSV. Tamb√©m √© poss√≠vel usar outra extens√£o como `.json` ou `.jsonlines`. Isso facilita a an√°lise do que est√° sendo raspado.

- `-s LOG_FILE=logs.txt`: Salva os resultados do log em um arquivo texto. Se o log estiver muito grande, √© √∫til para que erros n√£o passem despercebidos.

- `-a start_date=2020-12-01`: Tamb√©m √© muito importante testar se o filtro de data no raspador est√° funcionando. Utilizando esse argumento, apenas as requisi√ß√µes necess√°rias para extrair documentos a partir da data desejada devem ser feitas. Este exemplo faz o teste para publica√ß√µes a partir de 1 de dezembro de 2020. O atributo `start_date` do raspador √© utilizado internamente, ent√£o, e o argumento n√£o for passado, o padr√£o (primeira data de publica√ß√£o) √© utilizado no lugar.

Para rodar o comando usando todas a op√ß√µes anteriores em `sp_paulinia`, usar√≠amos o seguinte comando:

```
scrapy crawl sp_paulinia -a start_date=2020-12-01 -s FILES_STORE="" -s LOG_FILE=logs.txt -o output.json
```

## Enviando sua contribui√ß√£o ü§ù

Ao fazer o commit do c√≥digo, mencione a issue do raspador da sua cidade. Voc√™ pode incluir uma mensagem como `Close #20`, por exemplo, onde #20 √© o n√∫mero identificador da issue criada. Tamb√©m adicione uma descri√ß√£o comentando suas op√ß√µes na hora de desenvolver o raspador ou eventuais incertezas.

Normalmente adicionar apenas um raspador necessita apenas de um √∫nico commit. Mas, se for necess√°rio mais de um commit, tente manter um certo n√≠vel de separa√ß√£o entre o que cada um est√° fazendo e tamb√©m se certifique que suas mensagens est√£o bem claras e correspondendo ao que os commits realmente fazem.

Uma boa pr√°tica √© sempre atualizar a ramifica√ß√£o (_branch_) que voc√™ est√° desenvolvendo com o que est√° na `main` atualizada do projeto. Assim, se o projeto teve atualiza√ß√µes, voc√™ pode resolver algum conflito antes mesmo de fazer o Pull Request.

Qualquer d√∫vida, abra o seu Pull Request em modo de rascunho (_draft_) e relate suas d√∫vidas para que pessoas do projeto tentem te ajudar üòÉ. O [canal de discuss√µes no Discord](https://discord.com/invite/nDc9p4drm4) tamb√©m √© aberto para tirar d√∫vidas e trocar ideias.
